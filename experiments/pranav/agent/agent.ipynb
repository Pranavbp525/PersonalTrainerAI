{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index_name = \"fitness-chatbot\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Matches embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "@tool\n",
    "def retrieve_exercise_info(query: str) -> str:\n",
    "    \"\"\"Retrieves science-based exercise information from Pinecone vector store.\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
    "    retrieved_docs = [match[\"metadata\"].get(\"text\", \"No text available\") for match in results[\"matches\"]]\n",
    "    return \"Retrieved documents:\\n\" + \"\\n\".join(retrieved_docs)  # Modified to prefix with label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "llm_with_tools = llm.bind_tools([retrieve_exercise_info, \n",
    "                                 tool_fetch_workouts,\n",
    "                                 tool_get_workout_count,\n",
    "                                 tool_fetch_routines,\n",
    "                                 tool_update_routine,\n",
    "                                 tool_create_routine,\n",
    "                                 retrieve_from_rag])\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[dict], add_messages]\n",
    "    session_id: str\n",
    "    tool_results: dict\n",
    "\n",
    "def chatbot(state: AgentState) -> dict:\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=[retrieve_exercise_info]))\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "app = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5919ceecec6465ab2796e03572ceee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', placeholder='Type your message here...'), Button(description='Sen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create widgets\n",
    "input_box = widgets.Text(placeholder='Type your message here...')\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "output_area = Output()\n",
    "\n",
    "# Layout\n",
    "chat_ui = widgets.VBox([\n",
    "    widgets.HBox([input_box, send_button]),\n",
    "    output_area\n",
    "])\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    with output_area:\n",
    "        print(f\"User: {user_input}\")\n",
    "        for event in app.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "            for value in event.values():\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "def on_send_clicked(b):\n",
    "    user_input = input_box.value\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        with output_area:\n",
    "            print(\"Goodbye!\")\n",
    "        return\n",
    "    \n",
    "    stream_graph_updates(user_input)\n",
    "    input_box.value = ''  # Clear input box\n",
    "\n",
    "# Set up event handler\n",
    "send_button.on_click(on_send_clicked)\n",
    "\n",
    "# Display the chat interface\n",
    "display(chat_ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "**1.** Check the retrieved info carefully and see if the agent is inferring from retrieved info or using its existing knowledge.\n",
    "\n",
    "**2:** If its using existing knowledge, then prompt engineer to prevent that.\n",
    "\n",
    "**3:** Check quality of retrieved Info. Iterate on diff RAG Techniques to compare the quality.\n",
    "\n",
    "**4:** If none of them give a good enough quality, try GraphRAG.\n",
    "\n",
    "**5:** If it still is not good enough quality, try DeepResearch on RAG.\n",
    "\n",
    "**6:** Final Fallback to web deepresearch.\n",
    "\n",
    "**7:** Implement rest of the tools.\n",
    "\n",
    "**8:** Repeat all the quality check steps for rest of the tools, and when everything is passed, then quality check on the whole agent.\n",
    "\n",
    "**9:** Integrate the final passed Agent into the chatbot backend.\n",
    "\n",
    "**10:** Incorporate tracking, tracing, prompt versioning etc into the developed agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavtandra/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional, Literal, Union\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables and initialize services as you already have\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index_name = \"fitness-chatbot\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Matches embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "@tool\n",
    "def retrieve_exercise_info(query: str) -> str:\n",
    "    \"\"\"Retrieves science-based exercise information from Pinecone vector store.\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
    "    retrieved_docs = [match[\"metadata\"].get(\"text\", \"No text available\") for match in results[\"matches\"]]\n",
    "    return \"Retrieved documents:\\n\" + \"\\n\".join(retrieved_docs)  # Modified to prefix with label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "llm_with_tools = llm.bind_tools([retrieve_exercise_info, \n",
    "                                 tool_fetch_workouts,\n",
    "                                 tool_get_workout_count,\n",
    "                                 tool_fetch_routines,\n",
    "                                 tool_update_routine,\n",
    "                                 tool_create_routine,\n",
    "                                 retrieve_from_rag])\n",
    "\n",
    "\n",
    "\n",
    "# Define the agent state to track conversation and fitness journey\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    user_profile: Optional[dict]\n",
    "    assessment_complete: bool\n",
    "    workout_plan: Optional[dict]\n",
    "    progress_data: Optional[dict]\n",
    "    next_step: Literal[\"assessment\", \"knowledge_retrieval\", \"plan_generation\", \n",
    "                      \"progress_analysis\", \"plan_adjustment\", \"communication\", \"end\"]\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Node 1: Assessment - Collects user information\n",
    "def assessment_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Handle initial user assessment or periodic reassessment.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert personal trainer with deep knowledge of exercise science.\n",
    "    Your task is to assess the user to create a personalized workout plan.\n",
    "    \n",
    "    If this is the first interaction, ask about:\n",
    "    - Current fitness level and experience with weight training\n",
    "    - Specific fitness goals (weight loss, muscle gain, strength, etc.)\n",
    "    - Available workout days and time per session\n",
    "    - Equipment access (home gym, commercial gym, minimal equipment)\n",
    "    - Any injuries or health concerns\n",
    "    - Age, height, and weight (if comfortable sharing)\n",
    "    \n",
    "    If this is a reassessment, review their progress and ask about:\n",
    "    - Satisfaction with current workout plan\n",
    "    - Any changes to goals or constraints\n",
    "    - Any new injuries or limitations\n",
    "    \n",
    "    Ask questions one at a time to avoid overwhelming the user.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=system_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Check if assessment is complete based on conversation\n",
    "    # This is simplified - in practice you'd want more sophisticated detection\n",
    "    assessment_complete = False\n",
    "    if state.get(\"user_profile\") and all(k in state[\"user_profile\"] for k in \n",
    "                                        [\"goals\", \"fitness_level\", \"available_time\", \"equipment\"]):\n",
    "        assessment_complete = True\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\", {}),\n",
    "        \"assessment_complete\": assessment_complete,\n",
    "        \"next_step\": \"communication\"\n",
    "    }\n",
    "\n",
    "# Node 2: Knowledge Retrieval - Gets relevant fitness information from RAG\n",
    "def knowledge_retrieval_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Query the GraphRAG knowledge base for relevant fitness information.\"\"\"\n",
    "    user_profile = state.get(\"user_profile\", {})\n",
    "    \n",
    "    # Create relevant queries based on user profile\n",
    "    queries = [\n",
    "        f\"Recommended exercises for {user_profile.get('goals', 'general fitness')}\",\n",
    "        f\"Workout structure for {user_profile.get('fitness_level', 'beginner')} level\",\n",
    "        f\"Training frequency for {user_profile.get('goals', 'muscle building')}\"\n",
    "    ]\n",
    "    \n",
    "    # Get information from RAG for each query\n",
    "    retrieved_info = []\n",
    "    for query in queries:\n",
    "        result = retrieve_from_rag(query)\n",
    "        retrieved_info.append(f\"Query: {query}\\nResults: {result}\")\n",
    "    \n",
    "    knowledge_summary = \"\\n\\n\".join(retrieved_info)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": user_profile,\n",
    "        \"assessment_complete\": state.get(\"assessment_complete\", False),\n",
    "        \"retrieved_knowledge\": knowledge_summary,\n",
    "        \"next_step\": \"plan_generation\"\n",
    "    }\n",
    "\n",
    "# Node 3: Plan Generation - Creates workout routines\n",
    "def plan_generation_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Create a personalized workout plan based on assessment and knowledge.\"\"\"\n",
    "    user_profile = state.get(\"user_profile\", {})\n",
    "    knowledge = state.get(\"retrieved_knowledge\", \"\")\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    Using your expertise as a personal trainer and the fitness knowledge retrieved,\n",
    "    create a detailed workout plan for the user.\n",
    "    \n",
    "    User profile:\n",
    "    - Goals: {user_profile.get(\"goals\", \"Not specified\")}\n",
    "    - Fitness Level: {user_profile.get(\"fitness_level\", \"Not specified\")}\n",
    "    - Available Time: {user_profile.get(\"available_time\", \"Not specified\")}\n",
    "    - Equipment: {user_profile.get(\"equipment\", \"Not specified\")}\n",
    "    \n",
    "    Retrieved science-based information:\n",
    "    {knowledge}\n",
    "    \n",
    "    Create a structured workout routine with:\n",
    "    1. Weekly schedule (which days to train)\n",
    "    2. Each workout session with:\n",
    "       - Specific exercises with proper form cues\n",
    "       - Sets, reps, and rest periods\n",
    "       - Progressive overload strategy\n",
    "    \n",
    "    Format the workout plan in a way that can be used to create routines in the Hevy app.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=system_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Here you'd parse the response to create a structured workout plan in Hevy format\n",
    "    # For simplicity, we'll just store the raw plan\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": user_profile,\n",
    "        \"assessment_complete\": True,\n",
    "        \"workout_plan\": {\"raw_plan\": response.content, \"created_date\": datetime.now().isoformat()},\n",
    "        \"next_step\": \"communication\"\n",
    "    }\n",
    "\n",
    "# Node 4: Progress Analysis - Tracks user's fitness journey\n",
    "def progress_analysis_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyze user's workout logs to track progress.\"\"\"\n",
    "    \n",
    "    # Fetch workout data from Hevy API\n",
    "    recent_workouts = tool_fetch_workouts(page=1, page_size=10)\n",
    "    workout_count = tool_get_workout_count()\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    As a personal trainer, analyze the user's recent workout data:\n",
    "    \n",
    "    Total workouts completed: {workout_count}\n",
    "    Recent workout logs: {json.dumps(recent_workouts, indent=2)}\n",
    "    \n",
    "    Analyze:\n",
    "    1. Workout consistency (frequency)\n",
    "    2. Progress in weight/reps over time\n",
    "    3. Exercise adherence to the plan\n",
    "    4. Potential plateaus or areas needing attention\n",
    "    5. Overall progress toward stated goals\n",
    "    \n",
    "    Provide a detailed analysis to inform plan adjustments.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    progress_data = {\n",
    "        \"workout_count\": workout_count,\n",
    "        \"recent_workouts\": recent_workouts,\n",
    "        \"analysis\": response.content,\n",
    "        \"analysis_date\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"assessment_complete\": state.get(\"assessment_complete\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": progress_data,\n",
    "        \"next_step\": \"plan_adjustment\"\n",
    "    }\n",
    "\n",
    "# Node 5: Plan Adjustment - Modifies workouts based on progress\n",
    "def plan_adjustment_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Adjust workout plan based on progress analysis.\"\"\"\n",
    "    user_profile = state.get(\"user_profile\", {})\n",
    "    current_plan = state.get(\"workout_plan\", {})\n",
    "    progress_data = state.get(\"progress_data\", {})\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    As an expert personal trainer, review the user's progress and current workout plan.\n",
    "    \n",
    "    User profile: {json.dumps(user_profile, indent=2)}\n",
    "    Current workout plan: {json.dumps(current_plan, indent=2)}\n",
    "    Progress analysis: {json.dumps(progress_data, indent=2)}\n",
    "    \n",
    "    Determine if the plan needs:\n",
    "    1. Minor tweaks (increasing weights/reps/sets)\n",
    "    2. Moderate changes (replacing some exercises)\n",
    "    3. Major overhaul (new routine based on progress or goal changes)\n",
    "    \n",
    "    Update the workout plan with science-based adjustments to ensure continued progress.\n",
    "    Format the adjusted plan for implementation in the Hevy app.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Extract adjusted workout plan from response\n",
    "    adjusted_plan = {\n",
    "        \"raw_plan\": response.content, \n",
    "        \"adjustment_date\": datetime.now().isoformat(),\n",
    "        \"previous_plan\": current_plan\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"assessment_complete\": state.get(\"assessment_complete\"),\n",
    "        \"workout_plan\": adjusted_plan,\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"next_step\": \"communication\"\n",
    "    }\n",
    "\n",
    "# Node 6: Communication - Handles user interactions\n",
    "def communication_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Handle user interactions and determine next steps.\"\"\"\n",
    "    last_user_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_message = msg.content\n",
    "            break\n",
    "    \n",
    "    # Extract key information from messages to update user profile\n",
    "    if not state.get(\"assessment_complete\", False):\n",
    "        # Attempt to build/update user profile from conversation\n",
    "        profile_extractor_prompt = f\"\"\"\n",
    "        Extract key fitness information from this conversation:\n",
    "        {json.dumps([m.content for m in state[\"messages\"]])}\n",
    "        \n",
    "        Return a JSON object with these fields:\n",
    "        - goals: User's fitness goals\n",
    "        - fitness_level: Beginner, intermediate, or advanced\n",
    "        - available_time: Weekly availability for workouts\n",
    "        - equipment: Available equipment\n",
    "        - limitations: Any injuries or constraints\n",
    "        \n",
    "        Only include fields where information is clearly provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        extraction_messages = [SystemMessage(content=profile_extractor_prompt)]\n",
    "        extraction_response = llm.invoke(extraction_messages)\n",
    "        \n",
    "        try:\n",
    "            # Parse profile information\n",
    "            extracted_profile = json.loads(extraction_response.content)\n",
    "            current_profile = state.get(\"user_profile\", {})\n",
    "            updated_profile = {**current_profile, **extracted_profile}\n",
    "            \n",
    "            # Check if we have enough information to complete assessment\n",
    "            required_fields = [\"goals\", \"fitness_level\", \"available_time\", \"equipment\"]\n",
    "            assessment_complete = all(field in updated_profile for field in required_fields)\n",
    "            \n",
    "            next_step = \"knowledge_retrieval\" if assessment_complete else \"assessment\"\n",
    "            \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"session_id\": state[\"session_id\"],\n",
    "                \"user_profile\": updated_profile,\n",
    "                \"assessment_complete\": assessment_complete,\n",
    "                \"next_step\": next_step\n",
    "            }\n",
    "        except:\n",
    "            # If parsing fails, continue with assessment\n",
    "            next_step = \"assessment\"\n",
    "    \n",
    "    # If we have a complete profile but no workout plan, generate one\n",
    "    elif state.get(\"assessment_complete\") and not state.get(\"workout_plan\"):\n",
    "        next_step = \"knowledge_retrieval\"\n",
    "    \n",
    "    # If we have a workout plan but no recent progress analysis, check progress\n",
    "    elif state.get(\"workout_plan\") and (not state.get(\"progress_data\") or \n",
    "            (datetime.now() - datetime.fromisoformat(state.get(\"progress_data\", {}).get(\"analysis_date\", datetime.now().isoformat()))).days > 7):\n",
    "        next_step = \"progress_analysis\"\n",
    "    \n",
    "    # Default to ending the interaction\n",
    "    else:\n",
    "        next_step = \"end\"\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are a knowledgeable, supportive personal trainer. \n",
    "    Be conversational but professional.\n",
    "    \n",
    "    User has a fitness profile: {\"Yes\" if state.get(\"user_profile\") else \"No\"}\n",
    "    Assessment complete: {\"Yes\" if state.get(\"assessment_complete\", False) else \"No\"}\n",
    "    Workout plan exists: {\"Yes\" if state.get(\"workout_plan\") else \"No\"}\n",
    "    Progress data available: {\"Yes\" if state.get(\"progress_data\") else \"No\"}\n",
    "    \n",
    "    Respond to the user's last message. If they ask about their workout plan, explain it.\n",
    "    If they share results, provide encouragement. If they have questions, answer using\n",
    "    scientific principles from exercise science.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=system_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"assessment_complete\": state.get(\"assessment_complete\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"next_step\": next_step\n",
    "    }\n",
    "\n",
    "# Router function to determine the next node\n",
    "def router(state: AgentState) -> str:\n",
    "    \"\"\"Route to the next node based on the state.\"\"\"\n",
    "    return state[\"next_step\"]\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"assessment\", assessment_node)\n",
    "workflow.add_node(\"knowledge_retrieval\", knowledge_retrieval_node)\n",
    "workflow.add_node(\"plan_generation\", plan_generation_node)\n",
    "workflow.add_node(\"progress_analysis\", progress_analysis_node)\n",
    "workflow.add_node(\"plan_adjustment\", plan_adjustment_node)\n",
    "workflow.add_node(\"communication\", communication_node)\n",
    "workflow.add_node(\"tools\", ToolNode([\n",
    "    retrieve_exercise_info, \n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    "]))\n",
    "\n",
    "# Add edges between nodes\n",
    "workflow.add_conditional_edges(\n",
    "    \"communication\",\n",
    "    router,\n",
    "    {\n",
    "        \"assessment\": \"assessment\",\n",
    "        \"knowledge_retrieval\": \"knowledge_retrieval\",\n",
    "        \"plan_generation\": \"plan_generation\",\n",
    "        \"progress_analysis\": \"progress_analysis\",\n",
    "        \"plan_adjustment\": \"plan_adjustment\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"assessment\",\n",
    "    router,\n",
    "    {\n",
    "        \"knowledge_retrieval\": \"knowledge_retrieval\",\n",
    "        \"communication\": \"communication\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"knowledge_retrieval\",\n",
    "    router,\n",
    "    {\n",
    "        \"plan_generation\": \"plan_generation\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"plan_generation\",\n",
    "    router,\n",
    "    {\n",
    "        \"communication\": \"communication\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"progress_analysis\",\n",
    "    router,\n",
    "    {\n",
    "        \"plan_adjustment\": \"plan_adjustment\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"plan_adjustment\",\n",
    "    router,\n",
    "    {\n",
    "        \"communication\": \"communication\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add conditional edges for tool usage\n",
    "for node in [\"assessment\", \"knowledge_retrieval\", \"plan_generation\", \n",
    "             \"progress_analysis\", \"plan_adjustment\", \"communication\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        tools_condition,\n",
    "        {\n",
    "            True: \"tools\",\n",
    "            False: node  # Stay in the same node if no tools needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "workflow.add_edge(\"tools\", \"communication\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"communication\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Function to run the agent with user input\n",
    "# def run_fitness_coach(user_input: str, session_id: str, state=None):\n",
    "#     \"\"\"Run the personal trainer agent with user input.\"\"\"\n",
    "#     if state is None:\n",
    "#         initial_state = {\n",
    "#             \"messages\": [HumanMessage(content=user_input)],\n",
    "#             \"session_id\": session_id,\n",
    "#             \"next_step\": \"communication\"\n",
    "#         }\n",
    "#     else:\n",
    "#         # Add the new message to existing state\n",
    "#         state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "#         initial_state = state\n",
    "    \n",
    "#     # Run the workflow\n",
    "#     final_state = app.invoke(initial_state)\n",
    "#     return final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tassessment(assessment)\n",
      "\tknowledge_retrieval(knowledge_retrieval)\n",
      "\tplan_generation(plan_generation)\n",
      "\tprogress_analysis(progress_analysis)\n",
      "\tplan_adjustment(plan_adjustment)\n",
      "\tcommunication(communication)\n",
      "\ttools(tools)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> communication;\n",
      "\ttools --> communication;\n",
      "\tcommunication -.-> assessment;\n",
      "\tcommunication -.-> knowledge_retrieval;\n",
      "\tcommunication -.-> plan_generation;\n",
      "\tcommunication -.-> progress_analysis;\n",
      "\tcommunication -.-> plan_adjustment;\n",
      "\tcommunication -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tcommunication -. &nbsp;True&nbsp; .-> tools;\n",
      "\tassessment -.-> knowledge_retrieval;\n",
      "\tassessment -.-> communication;\n",
      "\tassessment -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tassessment -. &nbsp;True&nbsp; .-> tools;\n",
      "\tknowledge_retrieval -.-> plan_generation;\n",
      "\tknowledge_retrieval -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tknowledge_retrieval -. &nbsp;True&nbsp; .-> tools;\n",
      "\tplan_generation -.-> communication;\n",
      "\tplan_generation -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tplan_generation -. &nbsp;True&nbsp; .-> tools;\n",
      "\tprogress_analysis -.-> plan_adjustment;\n",
      "\tprogress_analysis -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tprogress_analysis -. &nbsp;True&nbsp; .-> tools;\n",
      "\tplan_adjustment -.-> communication;\n",
      "\tplan_adjustment -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tplan_adjustment -. &nbsp;True&nbsp; .-> tools;\n",
      "\tcommunication -. &nbsp;False&nbsp; .-> communication;\n",
      "\tassessment -. &nbsp;False&nbsp; .-> assessment;\n",
      "\tknowledge_retrieval -. &nbsp;False&nbsp; .-> knowledge_retrieval;\n",
      "\tplan_generation -. &nbsp;False&nbsp; .-> plan_generation;\n",
      "\tprogress_analysis -. &nbsp;False&nbsp; .-> progress_analysis;\n",
      "\tplan_adjustment -. &nbsp;False&nbsp; .-> plan_adjustment;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mermaid_diagram = app.get_graph().draw_mermaid()\n",
    "print(mermaid_diagram)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Assuming app.get_graph().draw_mermaid_png() returns a URL:\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m image_content \u001b[38;5;241m=\u001b[39m get_mermaid_image(image_url)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_content:\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/langchain_core/runnables/graph.py:630\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    625\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    626\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[1;32m    627\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[1;32m    628\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    629\u001b[0m )\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/langchain_core/runnables/graph_mermaid.py:214\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    208\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    209\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    210\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 214\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/langchain_core/runnables/graph_mermaid.py:336\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[0m\n\u001b[1;32m    330\u001b[0m         background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m image_url \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    338\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/PersonalTrainerAI/venv/lib/python3.9/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import Image, display\n",
    "import requests\n",
    "\n",
    "def get_mermaid_image(url, retries=3, timeout=20):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return None\n",
    "\n",
    "# Assuming app.get_graph().draw_mermaid_png() returns a URL:\n",
    "image_url = app.get_graph().draw_mermaid_png()\n",
    "image_content = get_mermaid_image(image_url)\n",
    "\n",
    "if image_content:\n",
    "    display(Image(image_content))\n",
    "else:\n",
    "    print(\"Failed to retrieve the Mermaid diagram after several attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'mermaid_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diagram_code \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmermaid_code\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'mermaid_code'"
     ]
    }
   ],
   "source": [
    "diagram_code = app.get_graph().mermaid_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'to_mermaid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mermaid_diagram \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_mermaid\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(mermaid_diagram)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'to_mermaid'"
     ]
    }
   ],
   "source": [
    "mermaid_diagram = app.get_graph().to_mermaid()\n",
    "print(mermaid_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "# Load environment variables and initialize services\n",
    "load_dotenv()\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create the system prompt with detailed instructions\n",
    "system_prompt = \"\"\"You are an expert personal fitness trainer with deep knowledge of exercise science.\n",
    "\n",
    "Your capabilities:\n",
    "1. Assessment: You can ask relevant questions to understand the user's fitness level, goals, and constraints\n",
    "2. Knowledge: You have access to exercise science information through the retrieve_from_rag tool\n",
    "3. Workout Planning: You can create personalized routines using scientific principles\n",
    "4. Progress Tracking: You can analyze workout logs from Hevy using the tool_fetch_workouts tool\n",
    "5. Plan Adjustment: You can modify workouts based on progress and feedback\n",
    "\n",
    "When working with a new user:\n",
    "- Ask about their fitness goals, experience level, available equipment, and schedule\n",
    "- Be conversational but thorough in your assessment\n",
    "- Use retrieve_from_rag to get scientific information about appropriate exercises\n",
    "- Create a personalized workout plan\n",
    "- Use tool_create_routine to save the workout to their Hevy account\n",
    "\n",
    "For returning users:\n",
    "- Check their progress using tool_fetch_workouts and tool_get_workout_count\n",
    "- Analyze their adherence and performance\n",
    "- Suggest modifications to their routine as needed\n",
    "- Use tool_update_routine to modify existing routines\n",
    "\n",
    "Always provide scientific rationale for your recommendations and maintain a supportive, motivational tone.\n",
    "\"\"\"\n",
    "\n",
    "# Define the main agent node\n",
    "def fitness_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"The main fitness trainer agent that handles all aspects of the interaction.\"\"\"\n",
    "    # Add the system prompt for first-time messages\n",
    "    if len(state[\"messages\"]) == 1 and isinstance(state[\"messages\"][0], HumanMessage):\n",
    "        messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "    # Invoke the model with tools\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"tool_calls\": response.tool_calls if hasattr(response, \"tool_calls\") else None\n",
    "    }\n",
    "\n",
    "# Define the graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"fitness_agent\", fitness_agent)\n",
    "graph.add_node(\"tools\", ToolNode([\n",
    "    retrieve_exercise_info, \n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    "]))\n",
    "\n",
    "# Add edges\n",
    "graph.add_conditional_edges(\n",
    "    \"fitness_agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        True: \"tools\",\n",
    "        False: END\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"tools\", \"fitness_agent\")\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"fitness_agent\")\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tfitness_agent(fitness_agent)\n",
      "\ttools(tools)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> fitness_agent;\n",
      "\ttools --> fitness_agent;\n",
      "\tfitness_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tfitness_agent -. &nbsp;False&nbsp; .-> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mermaid_diagram = app.get_graph().draw_mermaid()\n",
    "print(mermaid_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    user_profile: Optional[dict]\n",
    "    workout_plan: Optional[dict]\n",
    "    progress_data: Optional[dict]\n",
    "    stage: Literal[\"assessment\", \"planning\", \"monitoring\"]\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Assessment stage - handles user profiling and goal setting\n",
    "def assessment_stage(state: AgentState) -> AgentState:\n",
    "    assessment_prompt = \"\"\"You are a fitness expert in the assessment phase.\n",
    "    Ask targeted questions about the user's:\n",
    "    1. Fitness goals (strength, endurance, weight loss, etc.)\n",
    "    2. Training history and current fitness level\n",
    "    3. Available equipment and schedule\n",
    "    4. Any health concerns or limitations\n",
    "    \n",
    "    Use the information to build a comprehensive user profile.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=assessment_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Extract profile information from conversation if possible\n",
    "    # (code for profile extraction would go here)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\", {}),\n",
    "        \"stage\": \"planning\" if state.get(\"user_profile\") else \"assessment\"\n",
    "    }\n",
    "\n",
    "# Planning stage - creates workout routines\n",
    "def planning_stage(state: AgentState) -> AgentState:\n",
    "    planning_prompt = \"\"\"You are a fitness expert in the planning phase.\n",
    "    Use the user profile and retrieve_from_rag to create a personalized workout plan.\n",
    "    Include:\n",
    "    1. Weekly schedule breakdown\n",
    "    2. Specific exercises with sets, reps, and rest periods\n",
    "    3. Progression scheme over time\n",
    "    4. Format the plan for Hevy app integration\n",
    "    \n",
    "    Explain the scientific rationale behind each element of the plan.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=planning_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": {\"content\": response.content, \"created\": \"now\"},\n",
    "        \"stage\": \"monitoring\"\n",
    "    }\n",
    "\n",
    "# Monitoring stage - tracks progress and adjusts plans\n",
    "def monitoring_stage(state: AgentState) -> AgentState:\n",
    "    monitoring_prompt = \"\"\"You are a fitness expert in the monitoring phase.\n",
    "    Analyze the user's workout logs from the Hevy API and:\n",
    "    1. Track adherence to the workout plan\n",
    "    2. Identify progress or plateaus\n",
    "    3. Suggest adjustments to the plan as needed\n",
    "    4. Provide motivation and science-based advice\n",
    "    \n",
    "    Use tool_fetch_workouts to view recent activity.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=monitoring_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"stage\": \"monitoring\"  # Stay in monitoring once we reach it\n",
    "    }\n",
    "\n",
    "# Stage router\n",
    "def stage_router(state: AgentState) -> str:\n",
    "    return state[\"stage\"]\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"assessment\", assessment_stage)\n",
    "workflow.add_node(\"planning\", planning_stage)\n",
    "workflow.add_node(\"monitoring\", monitoring_stage)\n",
    "workflow.add_node(\"tools\", ToolNode([retrieve_from_rag, tool_fetch_workouts]))\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"assessment\",\n",
    "    stage_router,\n",
    "    {\n",
    "        \"planning\": \"planning\"\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"planning\",\n",
    "    stage_router,\n",
    "    {\n",
    "        \"monitoring\": \"monitoring\"\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"monitoring\",\n",
    "    stage_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",  # Allow returning to planning for major updates\n",
    "        \"end\": END  # Allow ending the conversation\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add tool usage edges\n",
    "for node in [\"assessment\", \"planning\", \"monitoring\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        tools_condition,\n",
    "        {\n",
    "            True: \"tools\",\n",
    "            False: node\n",
    "        }\n",
    "    )\n",
    "\n",
    "workflow.add_conditional_edges(\"tools\",\n",
    "                               lambda state: state['stage'],\n",
    "                               {\n",
    "                                   \"assessment\":\"assessment\",\n",
    "                                   \"planning\":\"planning\",\n",
    "                                   \"monitoring\":\"monitoring\"\n",
    "                               })\n",
    "\n",
    "workflow.set_entry_point(\"assessment\")\n",
    "\n",
    "level2_app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "# Define the agent state with more detailed tracking\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    user_profile: Optional[dict]\n",
    "    workout_plan: Optional[dict]\n",
    "    progress_data: Optional[dict]\n",
    "    training_history: Optional[List[Dict]]\n",
    "    analysis_results: Optional[Dict]\n",
    "    next_action: Literal[\"orchestrate\", \"assess\", \"research\", \"plan\", \"analyze\", \"adjust\", \"end\"]\n",
    "    context: Dict[str, Any]  # For sharing context between nodes\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Orchestrator - central coordinator that decides workflow\n",
    "def orchestrator_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Determines which specialized node should handle the current interaction.\"\"\"\n",
    "    \n",
    "    orchestrator_prompt = \"\"\"You are the coordinator of a fitness training system.\n",
    "    Based on the conversation history and user needs, determine the next appropriate action:\n",
    "    \n",
    "    - \"assess\": When we need to gather information about the user\n",
    "    - \"research\": When we need to retrieve exercise science information\n",
    "    - \"plan\": When we need to create a new workout routine\n",
    "    - \"analyze\": When we need to analyze workout data and progress\n",
    "    - \"adjust\": When we need to modify an existing workout plan\n",
    "    - \"end\": When the interaction is complete\n",
    "    \n",
    "    Consider the user's profile completeness, workout plan existence, and recent progress.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add thinking context for the orchestrator\n",
    "    context_summary = {\n",
    "        \"has_profile\": bool(state.get(\"user_profile\")),\n",
    "        \"has_workout_plan\": bool(state.get(\"workout_plan\")),\n",
    "        \"has_progress_data\": bool(state.get(\"progress_data\")),\n",
    "        \"training_history_count\": len(state.get(\"training_history\", [])),\n",
    "        \"last_action\": state.get(\"next_action\", \"orchestrate\")\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=orchestrator_prompt),\n",
    "        HumanMessage(content=f\"Context: {context_summary}\\n\\nDecide the next action based on the conversation history: {state['messages'][-1].content if state['messages'] else 'Initial interaction'}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Extract the next action from the response\n",
    "    action_mapping = {\n",
    "        \"assess\": \"assess\",\n",
    "        \"research\": \"research\",\n",
    "        \"plan\": \"plan\", \n",
    "        \"analyze\": \"analyze\",\n",
    "        \"adjust\": \"adjust\",\n",
    "        \"end\": \"end\"\n",
    "    }\n",
    "    \n",
    "    # Simple parsing (in real system, would be more robust)\n",
    "    next_action = \"orchestrate\"  # Default\n",
    "    for action in action_mapping:\n",
    "        if action in response.content.lower():\n",
    "            next_action = action_mapping[action]\n",
    "            break\n",
    "    \n",
    "    # Update context with orchestrator's reasoning\n",
    "    context = state.get(\"context\", {})\n",
    "    context[\"orchestrator_reasoning\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"next_action\": next_action,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# Assessment Worker - gathers user information\n",
    "def assessment_worker(state: AgentState) -> AgentState:\n",
    "    \"\"\"Handles user assessment and profile building.\"\"\"\n",
    "    \n",
    "    assessment_prompt = \"\"\"You are a fitness assessment specialist.\n",
    "    Ask targeted questions to build a comprehensive user profile.\n",
    "    Focus on gathering missing information based on what you already know.\n",
    "    \n",
    "    Current profile: {{user_profile}}\n",
    "    \n",
    "    Areas to assess:\n",
    "    - Fitness goals and priorities\n",
    "    - Training experience and current level\n",
    "    - Available equipment and schedule\n",
    "    - Physical limitations or health concerns\n",
    "    - Measurement data (if willing to share)\n",
    "    \n",
    "    Be conversational but thorough.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Customize the prompt with current profile\n",
    "    filled_prompt = assessment_prompt.replace(\n",
    "        \"{{user_profile}}\", \n",
    "        str(state.get(\"user_profile\", \"No existing profile\"))\n",
    "    )\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # In a real system, we'd have logic to extract profile information\n",
    "    # and update the user_profile dictionary\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\", {}),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"next_action\": \"orchestrate\",  # Return to orchestrator\n",
    "        \"context\": state.get(\"context\", {})\n",
    "    }\n",
    "\n",
    "# Research Worker - retrieves exercise science information\n",
    "def research_worker(state: AgentState) -> AgentState:\n",
    "    \"\"\"Retrieves relevant exercise science information.\"\"\"\n",
    "    \n",
    "    research_prompt = \"\"\"You are a fitness research specialist.\n",
    "    Based on the user's profile and current needs, retrieve relevant exercise science information.\n",
    "    Use the retrieve_from_rag tool to find scientific information about:\n",
    "    \n",
    "    - Optimal training approaches for the user's goals\n",
    "    - Exercise selection based on equipment and limitations\n",
    "    - Evidence-based progression schemes\n",
    "    - Recovery and periodization strategies\n",
    "    \n",
    "    Organize your findings in a clear, structured way.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = state.get(\"context\", {})\n",
    "    research_needed = context.get(\"research_topics\", [\"general training principles\"])\n",
    "    \n",
    "    # Add research topics to prompt\n",
    "    research_prompt += f\"\\n\\nSpecific topics to research: {', '.join(research_needed)}\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=research_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update context with research findings\n",
    "    context[\"research_findings\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"next_action\": \"orchestrate\",\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# Planning Worker - creates workout routines\n",
    "def planning_worker(state: AgentState) -> AgentState:\n",
    "    \"\"\"Creates personalized workout plans.\"\"\"\n",
    "    \n",
    "    planning_prompt = \"\"\"You are a workout programming specialist.\n",
    "    Create a detailed, personalized workout plan for the user.\n",
    "    \n",
    "    User profile: {{user_profile}}\n",
    "    Research findings: {{research_findings}}\n",
    "    \n",
    "    Your plan should include:\n",
    "    1. Weekly schedule with workout days and focus areas\n",
    "    2. Detailed workout structure for each session\n",
    "       - Specific exercises with sets, reps, rest periods\n",
    "       - Warm-up routines and cool-down stretches\n",
    "       - Form cues and technique notes\n",
    "    3. Progression scheme for 4-6 weeks\n",
    "    4. Structured format ready for Hevy app integration\n",
    "    \n",
    "    Explain the scientific rationale behind your choices.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = state.get(\"context\", {})\n",
    "    \n",
    "    # Customize the prompt\n",
    "    filled_prompt = planning_prompt.replace(\n",
    "        \"{{user_profile}}\", \n",
    "        str(state.get(\"user_profile\", \"No profile available\"))\n",
    "    ).replace(\n",
    "        \"{{research_findings}}\",\n",
    "        context.get(\"research_findings\", \"No research findings available\")\n",
    "    )\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Create workout plan structure\n",
    "    workout_plan = {\n",
    "        \"content\": response.content,\n",
    "        \"created_date\": \"current_date\",\n",
    "        \"version\": 1.0,\n",
    "        \"hevy_routines\": []  # Would contain IDs of created Hevy routines\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": workout_plan,\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"next_action\": \"orchestrate\",\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# Analysis Worker - analyzes workout data\n",
    "def analysis_worker(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyzes workout logs and progress.\"\"\"\n",
    "    \n",
    "    analysis_prompt = \"\"\"You are a fitness data analyst.\n",
    "    Retrieve and analyze the user's workout logs to assess progress.\n",
    "    \n",
    "    Use these tools:\n",
    "    - tool_fetch_workouts: Get recent workout data\n",
    "    - tool_get_workout_count: Check overall workout volume\n",
    "    \n",
    "    Analyze:\n",
    "    1. Workout adherence and consistency\n",
    "    2. Progress in key metrics (weight, reps, sets)\n",
    "    3. Comparison to the workout plan\n",
    "    4. Potential plateaus or areas needing attention\n",
    "    \n",
    "    Provide objective analysis backed by data.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=analysis_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Structure the analysis results\n",
    "    analysis_results = {\n",
    "        \"analysis\": response.content,\n",
    "        \"date\": \"current_date\",\n",
    "        \"key_metrics\": {},  # Would contain extracted metrics\n",
    "        \"recommendations\": []  # Would contain extracted recommendations\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": state.get(\"workout_plan\"),\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"analysis_results\": analysis_results,\n",
    "        \"next_action\": \"orchestrate\",\n",
    "        \"context\": state.get(\"context\", {})\n",
    "    }\n",
    "\n",
    "# Adjustment Worker - modifies workout plans\n",
    "def adjustment_worker(state: AgentState) -> AgentState:\n",
    "    \"\"\"Adjusts workout plans based on analysis.\"\"\"\n",
    "    \n",
    "    adjustment_prompt = \"\"\"You are a workout optimization specialist.\n",
    "    Adjust the user's workout plan based on progress analysis.\n",
    "    \n",
    "    Current plan: {{workout_plan}}\n",
    "    Analysis results: {{analysis_results}}\n",
    "    User profile: {{user_profile}}\n",
    "    \n",
    "    Determine the appropriate level of adjustment:\n",
    "    1. Minor: Adjust weights, reps, or sets\n",
    "    2. Moderate: Substitute exercises or change training variables\n",
    "    3. Major: Create a new phase or approach\n",
    "    \n",
    "    Use tool_update_routine to implement changes in Hevy.\n",
    "    \n",
    "    Explain the scientific rationale for each adjustment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Customize the prompt\n",
    "    filled_prompt = adjustment_prompt.replace(\n",
    "        \"{{workout_plan}}\",\n",
    "        str(state.get(\"workout_plan\", \"No plan available\"))\n",
    "    ).replace(\n",
    "        \"{{analysis_results}}\",\n",
    "        str(state.get(\"analysis_results\", \"No analysis available\"))\n",
    "    ).replace(\n",
    "        \"{{user_profile}}\",\n",
    "        str(state.get(\"user_profile\", \"No profile available\"))\n",
    "    )\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update the workout plan with adjustments\n",
    "    current_plan = state.get(\"workout_plan\", {})\n",
    "    adjusted_plan = {\n",
    "        **current_plan,\n",
    "        \"adjusted_content\": response.content,\n",
    "        \"adjustment_date\": \"current_date\",\n",
    "        \"version\": current_plan.get(\"version\", 1.0) + 0.1\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"session_id\": state[\"session_id\"],\n",
    "        \"user_profile\": state.get(\"user_profile\"),\n",
    "        \"workout_plan\": adjusted_plan,\n",
    "        \"progress_data\": state.get(\"progress_data\"),\n",
    "        \"training_history\": state.get(\"training_history\", []),\n",
    "        \"next_action\": \"orchestrate\",\n",
    "        \"context\": state.get(\"context\", {})\n",
    "    }\n",
    "\n",
    "# Action router\n",
    "def action_router(state: AgentState) -> str:\n",
    "    return state[\"next_action\"]\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"orchestrator\", orchestrator_node)\n",
    "workflow.add_node(\"assess\", assessment_worker)\n",
    "workflow.add_node(\"research\", research_worker)\n",
    "workflow.add_node(\"plan\", planning_worker)\n",
    "workflow.add_node(\"analyze\", analysis_worker)\n",
    "workflow.add_node(\"adjust\", adjustment_worker)\n",
    "workflow.add_node(\"tools\", ToolNode([\n",
    "    retrieve_from_rag, \n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine\n",
    "]))\n",
    "\n",
    "# Add orchestrator routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"orchestrator\",\n",
    "    action_router,\n",
    "    {\n",
    "        \"assess\": \"assess\",\n",
    "        \"research\": \"research\",\n",
    "        \"plan\": \"plan\",\n",
    "        \"analyze\": \"analyze\",\n",
    "        \"adjust\": \"adjust\",\n",
    "        \"orchestrate\": \"orchestrator\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add return edges to orchestrator\n",
    "for node in [\"assess\", \"research\", \"plan\", \"analyze\", \"adjust\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        action_router,\n",
    "        {\n",
    "            \"orchestrate\": \"orchestrator\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Add tool usage for all nodes\n",
    "for node in [\"assess\", \"research\", \"plan\", \"analyze\", \"adjust\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        tools_condition,\n",
    "        {\n",
    "            True: \"tools\",\n",
    "            False: node\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Tool return edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    lambda state: state[\"next_action\"],\n",
    "    {\n",
    "        \"orchestrate\": \"orchestrator\",\n",
    "        \"assess\": \"assess\",\n",
    "        \"research\": \"research\",\n",
    "        \"plan\": \"plan\",\n",
    "        \"analyze\": \"analyze\",\n",
    "        \"adjust\": \"adjust\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"orchestrator\")\n",
    "\n",
    "level3_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_diagram = level2_app.get_graph().draw_mermaid()\n",
    "level3_diagram = level3_app.get_graph().draw_mermaid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 4 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional, Literal, Union, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import uuid\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "# Define more sophisticated state model\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    memory: Dict[str, Any]  # Long-term memory storage\n",
    "    working_memory: Dict[str, Any]  # Short-term contextual memory\n",
    "    user_model: Dict[str, Any]  # Comprehensive user model\n",
    "    fitness_plan: Dict[str, Any]  # Structured fitness plan\n",
    "    reasoning_trace: List[Dict[str, Any]]  # Traces of reasoning steps\n",
    "    agent_state: Dict[str, str]  # Current state of each agent\n",
    "    current_agent: str  # Currently active agent\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Memory manager for sophisticated state management\n",
    "def memory_manager(state: AgentState) -> AgentState:\n",
    "    \"\"\"Manages long-term and working memory, consolidating information and pruning as needed.\"\"\"\n",
    "    \n",
    "    memory_prompt = \"\"\"You are the memory manager for a fitness training system.\n",
    "    Review the conversation history and current agent states to:\n",
    "    \n",
    "    1. Identify key information that should be stored in long-term memory\n",
    "    2. Update the user model with new insights\n",
    "    3. Consolidate redundant information\n",
    "    4. Prune outdated or superseded information\n",
    "    5. Ensure critical context is available in working memory\n",
    "    \n",
    "    Current long-term memory: {{memory}}\n",
    "    Current user model: {{user_model}}\n",
    "    Current working memory: {{working_memory}}\n",
    "    \n",
    "    Return a structured update of what should be stored, updated, or removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = memory_prompt.replace(\"{{memory}}\", str(state.get(\"memory\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Parse and update memory structures\n",
    "    # This would include sophisticated logic to maintain memory integrity\n",
    "    \n",
    "    # For this example, we'll make a simple update\n",
    "    memory = state.get(\"memory\", {})\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    user_model = state.get(\"user_model\", {})\n",
    "    \n",
    "    # Extract recent messages for context\n",
    "    recent_exchanges = []\n",
    "    for msg in state[\"messages\"][-10:]:  # Last 10 messages\n",
    "        if isinstance(msg, HumanMessage) or isinstance(msg, AIMessage):\n",
    "            recent_exchanges.append({\"role\": \"user\" if isinstance(msg, HumanMessage) else \"assistant\", \n",
    "                                    \"content\": msg.content})\n",
    "    \n",
    "    # Update working memory with recent context\n",
    "    working_memory[\"recent_exchanges\"] = recent_exchanges\n",
    "    working_memory[\"last_updated\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Track agent interactions in memory\n",
    "    memory_key = f\"interaction_{datetime.now().isoformat()}\"\n",
    "    memory[memory_key] = {\n",
    "        \"agent_states\": state.get(\"agent_state\", {}),\n",
    "        \"current_agent\": state.get(\"current_agent\", \"coordinator\"),\n",
    "        \"user_intent\": working_memory.get(\"current_user_intent\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"memory\": memory,\n",
    "        \"working_memory\": working_memory,\n",
    "        \"user_model\": user_model\n",
    "    }\n",
    "\n",
    "# Reasoning engine for sophisticated decision-making\n",
    "def reasoning_engine(state: AgentState) -> AgentState:\n",
    "    \"\"\"Advanced reasoning to determine the optimal next steps and actions.\"\"\"\n",
    "    \n",
    "    reasoning_prompt = \"\"\"You are the reasoning engine for a fitness training system.\n",
    "    Analyze the current situation using sophisticated reasoning to determine:\n",
    "    \n",
    "    1. What is the user's current need or intent?\n",
    "    2. What information do we currently have and what's missing?\n",
    "    3. What potential approaches could address the user's needs?\n",
    "    4. What are the tradeoffs between different approaches?\n",
    "    5. What is the optimal next action given all constraints?\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Working memory: {{working_memory}}\n",
    "    Fitness plan: {{fitness_plan}}\n",
    "    \n",
    "    Think step by step and document your reasoning process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = reasoning_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{fitness_plan}}\", str(state.get(\"fitness_plan\", {})))\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Extract reasoning trace\n",
    "    reasoning_trace = state.get(\"reasoning_trace\", [])\n",
    "    reasoning_trace.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"reasoning\": response.content,\n",
    "        \"context\": {\n",
    "            \"current_agent\": state.get(\"current_agent\"),\n",
    "            \"working_memory_snapshot\": state.get(\"working_memory\", {})\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Determine next agent based on reasoning\n",
    "    # Extract key insights to guide agent selection\n",
    "    agent_selection = \"coordinator\"  # Default\n",
    "    \n",
    "    if \"assessment\" in response.content.lower() or \"profile\" in response.content.lower():\n",
    "        agent_selection = \"profiler_agent\"\n",
    "    elif \"research\" in response.content.lower() or \"knowledge\" in response.content.lower():\n",
    "        agent_selection = \"research_agent\"\n",
    "    elif \"plan\" in response.content.lower() or \"routine\" in response.content.lower():\n",
    "        agent_selection = \"planner_agent\"\n",
    "    elif \"progress\" in response.content.lower() or \"analyze\" in response.content.lower():\n",
    "        agent_selection = \"analyst_agent\"\n",
    "    elif \"adjust\" in response.content.lower() or \"modify\" in response.content.lower():\n",
    "        agent_selection = \"adaptation_agent\"\n",
    "    elif \"motivate\" in response.content.lower() or \"coach\" in response.content.lower():\n",
    "        agent_selection = \"coach_agent\"\n",
    "    \n",
    "    # Update working memory with reasoning results\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"reasoning_summary\"] = response.content\n",
    "    working_memory[\"selected_agent\"] = agent_selection\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"reasoning_trace\": reasoning_trace,\n",
    "        \"current_agent\": agent_selection,\n",
    "        \"working_memory\": working_memory\n",
    "    }\n",
    "\n",
    "# User modeler for comprehensive user understanding\n",
    "def user_modeler(state: AgentState) -> AgentState:\n",
    "    \"\"\"Builds and maintains a comprehensive model of the user.\"\"\"\n",
    "    \n",
    "    modeling_prompt = \"\"\"You are a user modeling specialist for a fitness training system.\n",
    "    Analyze all available information about the user to build a comprehensive model:\n",
    "    \n",
    "    1. Extract explicit information (stated goals, preferences, constraints)\n",
    "    2. Infer implicit information (fitness level, motivation factors, learning style)\n",
    "    3. Identify gaps in our understanding that need to be addressed\n",
    "    4. Update confidence levels for different aspects of the model\n",
    "    \n",
    "    Current user model: {{user_model}}\n",
    "    Recent exchanges: {{recent_exchanges}}\n",
    "    \n",
    "    Return an updated user model with confidence scores for each attribute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = modeling_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{recent_exchanges}}\", \n",
    "                                         str(state.get(\"working_memory\", {}).get(\"recent_exchanges\", [])))\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Parse and update user model\n",
    "    # In a real system, we'd have sophisticated parsing and model updating\n",
    "    \n",
    "    # Simple example update\n",
    "    user_model = state.get(\"user_model\", {})\n",
    "    user_model[\"last_updated\"] = datetime.now().isoformat()\n",
    "    user_model[\"model_version\"] = user_model.get(\"model_version\", 0) + 1\n",
    "    \n",
    "    # Extract goals if mentioned in recent messages\n",
    "    for msg in state[\"messages\"][-5:]:\n",
    "        if isinstance(msg, HumanMessage) and \"goal\" in msg.content.lower():\n",
    "            user_model[\"has_explicit_goals\"] = True\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"user_model\": user_model\n",
    "    }\n",
    "\n",
    "# Agent coordinator for orchestrating the multi-agent system\n",
    "def coordinator_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Coordinates the multi-agent system and handles high-level decision making.\"\"\"\n",
    "    \n",
    "    coordinator_prompt = \"\"\"You are the coordinator for a fitness training multi-agent system.\n",
    "    \n",
    "    Current agent states:\n",
    "    {{agent_states}}\n",
    "    \n",
    "    Reasoning summary:\n",
    "    {{reasoning_summary}}\n",
    "    \n",
    "    Selected agent: {{selected_agent}}\n",
    "    \n",
    "    Your tasks:\n",
    "    1. Ensure smooth transitions between agents\n",
    "    2. Maintain conversation coherence across agent handoffs\n",
    "    3. Resolve conflicts between agent recommendations\n",
    "    4. Determine when to invoke specialized agents vs. handling directly\n",
    "    \n",
    "    If you're responding to the user directly, maintain a supportive, expert tone.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template with state information\n",
    "    filled_prompt = coordinator_prompt.replace(\"{{agent_states}}\", str(state.get(\"agent_state\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{reasoning_summary}}\", \n",
    "                                         str(state.get(\"working_memory\", {}).get(\"reasoning_summary\", \"\")))\n",
    "    filled_prompt = filled_prompt.replace(\"{{selected_agent}}\", \n",
    "                                         state.get(\"working_memory\", {}).get(\"selected_agent\", \"coordinator\"))\n",
    "    \n",
    "    # If user message is the last one, respond\n",
    "    last_message_is_user = False\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_message_is_user = True\n",
    "            break\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            last_message_is_user = False\n",
    "            break\n",
    "    \n",
    "    if last_message_is_user:\n",
    "        messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "            \"current_agent\": \"coordinator\"\n",
    "        }\n",
    "    else:\n",
    "        # Just update state without responding\n",
    "        return state\n",
    "\n",
    "# Specialized profiler agent for in-depth user assessment\n",
    "def profiler_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for in-depth user assessment and profiling.\"\"\"\n",
    "    \n",
    "    profiler_prompt = \"\"\"You are a fitness assessment and profiling specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Comprehensive fitness assessment methodology\n",
    "    2. Psychological profiling for motivation and adherence factors\n",
    "    3. Goal elicitation and refinement techniques\n",
    "    4. Learning style and communication preference assessment\n",
    "    \n",
    "    Ask targeted questions based on gaps in the user model.\n",
    "    Focus on building a complete picture of the user's needs and constraints.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = profiler_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"profiler_agent\"] = \"active\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"profiler_agent\",\n",
    "        \"agent_state\": agent_state\n",
    "    }\n",
    "\n",
    "# Research specialist agent for fitness knowledge\n",
    "def research_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for retrieving and synthesizing fitness knowledge.\"\"\"\n",
    "    \n",
    "    research_prompt = \"\"\"You are a fitness research and knowledge specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Working memory: {{working_memory}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Access to evidence-based fitness research\n",
    "    2. Critical analysis of fitness methodologies\n",
    "    3. Translation of scientific concepts to practical applications\n",
    "    4. Personalized recommendations based on research findings\n",
    "    \n",
    "    Use the retrieve_from_rag tool to access relevant scientific information.\n",
    "    Synthesize findings in a clear, actionable manner relevant to the user's needs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = research_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"research_agent\"] = \"active\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"research_agent\",\n",
    "        \"agent_state\": agent_state\n",
    "    }\n",
    "\n",
    "# Program design specialist agent\n",
    "def planner_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for creating sophisticated workout plans.\"\"\"\n",
    "    \n",
    "    planner_prompt = \"\"\"You are a fitness program design specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Working memory: {{working_memory}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Periodized program design based on sports science principles\n",
    "    2. Exercise selection optimization for specific goals\n",
    "    3. Progression modeling for continuous adaptation\n",
    "    4. Integration of recovery and volume management\n",
    "    \n",
    "    Create a scientifically-sound, personalized workout plan.\n",
    "    Format the plan for implementation in the Hevy app.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = planner_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = filled_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"planner_agent\"] = \"active\"\n",
    "    \n",
    "    # Store the workout plan\n",
    "    fitness_plan = state.get(\"fitness_plan\", {})\n",
    "    fitness_plan[\"plan_content\"] = response.content\n",
    "    fitness_plan[\"created_date\"] = datetime.now().isoformat()\n",
    "    fitness_plan[\"version\"] = fitness_plan.get(\"version\", 0) + 1\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"planner_agent\",\n",
    "        \"agent_state\": agent_state,\n",
    "        \"fitness_plan\": fitness_plan\n",
    "    }\n",
    "\n",
    "# Progress analysis specialist agent\n",
    "def analyst_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for analyzing workout data and progress.\"\"\"\n",
    "    \n",
    "    analyst_prompt = \"\"\"You are a fitness data analysis specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Fitness plan: {{fitness_plan}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Workout data analysis with statistical methods\n",
    "    2. Progress tracking across multiple dimensions\n",
    "    3. Pattern recognition in training responses\n",
    "    4. Predictive modeling for future progress\n",
    "    \n",
    "    Use tool_fetch_workouts to retrieve workout data.\n",
    "    Analyze the data for adherence, progress, and areas for improvement.\n",
    "    Provide data-driven insights and visualizations when appropriate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = analyst_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = analyst_prompt.replace(\"{{fitness_plan}}\", str(state.get(\"fitness_plan\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"analyst_agent\"] = \"active\"\n",
    "    \n",
    "    # Store analysis in working memory\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"latest_analysis\"] = {\n",
    "        \"content\": response.content,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"analyst_agent\",\n",
    "        \"agent_state\": agent_state,\n",
    "        \"working_memory\": working_memory\n",
    "    }\n",
    "\n",
    "# Plan adaptation specialist agent\n",
    "def adaptation_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for adapting workout plans based on progress and feedback.\"\"\"\n",
    "    \n",
    "    adaptation_prompt = \"\"\"You are a workout program adaptation specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Fitness plan: {{fitness_plan}}\n",
    "    Working memory: {{working_memory}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Intelligent workout modification based on progress data\n",
    "    2. Adaptation strategies for plateaus and setbacks\n",
    "    3. Progressive overload implementation\n",
    "    4. Personalization based on user feedback and preferences\n",
    "    \n",
    "    Analyze the latest progress data and user feedback.\n",
    "    Determine appropriate adaptations to the current fitness plan.\n",
    "    Use tool_update_routine to implement changes in the Hevy app.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = adaptation_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = adaptation_prompt.replace(\"{{fitness_plan}}\", str(state.get(\"fitness_plan\", {})))\n",
    "    filled_prompt = adaptation_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"adaptation_agent\"] = \"active\"\n",
    "    \n",
    "    # Update fitness plan with adaptations\n",
    "    fitness_plan = state.get(\"fitness_plan\", {})\n",
    "    fitness_plan[\"adaptation_history\"] = fitness_plan.get(\"adaptation_history\", [])\n",
    "    fitness_plan[\"adaptation_history\"].append({\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"changes\": response.content,\n",
    "        \"version\": fitness_plan.get(\"version\", 0) + 0.1\n",
    "    })\n",
    "    fitness_plan[\"version\"] = fitness_plan.get(\"version\", 0) + 0.1\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"adaptation_agent\",\n",
    "        \"agent_state\": agent_state,\n",
    "        \"fitness_plan\": fitness_plan\n",
    "    }\n",
    "\n",
    "# Motivational coaching specialist agent\n",
    "def coach_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"Specialized agent for motivational coaching and adherence support.\"\"\"\n",
    "    \n",
    "    coach_prompt = \"\"\"You are a motivational fitness coach and psychology specialist.\n",
    "    \n",
    "    User model: {{user_model}}\n",
    "    Working memory: {{working_memory}}\n",
    "    \n",
    "    Your specialized capabilities:\n",
    "    1. Behavioral psychology for workout adherence\n",
    "    2. Motivational interviewing techniques\n",
    "    3. Obstacle identification and mitigation strategies\n",
    "    4. Celebration of progress and achievement recognition\n",
    "    \n",
    "    Provide motivational support tailored to the user's mindset.\n",
    "    Use positive reinforcement while maintaining accountability.\n",
    "    Focus on building intrinsic motivation and self-efficacy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill template\n",
    "    filled_prompt = coach_prompt.replace(\"{{user_model}}\", str(state.get(\"user_model\", {})))\n",
    "    filled_prompt = coach_prompt.replace(\"{{working_memory}}\", str(state.get(\"working_memory\", {})))\n",
    "    \n",
    "    messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Update agent state\n",
    "    agent_state = state.get(\"agent_state\", {})\n",
    "    agent_state[\"coach_agent\"] = \"active\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "        \"current_agent\": \"coach_agent\",\n",
    "        \"agent_state\": agent_state\n",
    "    }\n",
    "\n",
    "# Agent router based on current_agent\n",
    "def agent_router(state: AgentState) -> str:\n",
    "    return state[\"current_agent\"]\n",
    "\n",
    "# Build the multi-agent graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for all agents and components\n",
    "workflow.add_node(\"memory_manager\", memory_manager)\n",
    "workflow.add_node(\"reasoning_engine\", reasoning_engine)\n",
    "workflow.add_node(\"user_modeler\", user_modeler)\n",
    "workflow.add_node(\"coordinator\", coordinator_agent)\n",
    "workflow.add_node(\"profiler_agent\", profiler_agent)\n",
    "workflow.add_node(\"research_agent\", research_agent)\n",
    "workflow.add_node(\"planner_agent\", planner_agent)\n",
    "workflow.add_node(\"analyst_agent\", analyst_agent)\n",
    "workflow.add_node(\"adaptation_agent\", adaptation_agent)\n",
    "workflow.add_node(\"coach_agent\", coach_agent)\n",
    "workflow.add_node(\"tools\", ToolNode([\n",
    "    retrieve_from_rag, \n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine\n",
    "]))\n",
    "\n",
    "# Define the flow through the system\n",
    "# 1. Start with the memory manager to update state\n",
    "workflow.add_edge(\"memory_manager\", \"user_modeler\")\n",
    "\n",
    "# 2. Update user model before reasoning\n",
    "workflow.add_edge(\"user_modeler\", \"reasoning_engine\")\n",
    "\n",
    "# 3. Reasoning engine selects the appropriate agent\n",
    "workflow.add_conditional_edges(\n",
    "    \"reasoning_engine\",\n",
    "    agent_router,\n",
    "    {\n",
    "        \"coordinator\": \"coordinator\",\n",
    "        \"profiler_agent\": \"profiler_agent\",\n",
    "        \"research_agent\": \"research_agent\",\n",
    "        \"planner_agent\": \"planner_agent\",\n",
    "        \"analyst_agent\": \"analyst_agent\",\n",
    "        \"adaptation_agent\": \"adaptation_agent\",\n",
    "        \"coach_agent\": \"coach_agent\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. All specialized agents return to the coordinator\n",
    "for agent in [\"profiler_agent\", \"research_agent\", \"planner_agent\", \n",
    "              \"analyst_agent\", \"adaptation_agent\", \"coach_agent\"]:\n",
    "    workflow.add_edge(agent, \"coordinator\")\n",
    "\n",
    "# 5. Coordinator completes the interaction or continues\n",
    "workflow.add_conditional_edges(\n",
    "    \"coordinator\",\n",
    "    lambda state: \"end\" if state.get(\"working_memory\", {}).get(\"should_end\", False) else \"memory_manager\",\n",
    "    {\n",
    "        \"memory_manager\": \"memory_manager\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# 6. Add tool usage for all agents\n",
    "for agent in [\"coordinator\", \"profiler_agent\", \"research_agent\", \"planner_agent\", \n",
    "              \"analyst_agent\", \"adaptation_agent\", \"coach_agent\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        agent,\n",
    "        tools_condition,\n",
    "        {\n",
    "            True: \"tools\",\n",
    "            False: agent  # No-op if no tool usage\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 7. Tool returns to the agent that called it\n",
    "workflow.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    agent_router,\n",
    "    {\n",
    "        \"coordinator\": \"coordinator\",\n",
    "        \"profiler_agent\": \"profiler_agent\",\n",
    "        \"research_agent\": \"research_agent\",\n",
    "        \"planner_agent\": \"planner_agent\", \n",
    "        \"analyst_agent\": \"analyst_agent\",\n",
    "        \"adaptation_agent\": \"adaptation_agent\",\n",
    "        \"coach_agent\": \"coach_agent\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"memory_manager\")\n",
    "\n",
    "# Compile the graph\n",
    "level4_app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 5 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, Optional, Literal, Union, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import uuid\n",
    "from pydantic import BaseModel, Field\n",
    "from llm_tools import (\n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine,\n",
    "    retrieve_from_rag\n",
    ")\n",
    "\n",
    "# Advanced state model with cognitive architecture layers\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    session_id: str\n",
    "    episodic_memory: Dict[str, Any]  # Memory of interactions and events\n",
    "    semantic_memory: Dict[str, Any]  # Conceptual knowledge and facts\n",
    "    working_memory: Dict[str, Any]  # Active short-term memory\n",
    "    user_model: Dict[str, Any]  # Comprehensive user model\n",
    "    fitness_domain: Dict[str, Any]  # Domain-specific knowledge\n",
    "    metacognition: Dict[str, Any]  # System's awareness of its own state and performance\n",
    "    current_plan: Dict[str, Any]  # Current workout or interaction plan\n",
    "    execution_trace: List[Dict]  # Record of actions and outcomes\n",
    "    reflection_log: List[Dict]  # Self-evaluation and improvement notes\n",
    "    controller_state: str  # Current state of the hierarchical controller\n",
    "    error_state: Optional[Dict]  # Current error state, if any\n",
    "    human_feedback: Optional[Dict]  # Explicit feedback from human user\n",
    "    tool_calls: Optional[List[Dict]]\n",
    "    tool_results: Optional[Dict]\n",
    "\n",
    "# Controller states\n",
    "CONTROLLER_STATES = Literal[\n",
    "    \"perception\", \"interpretation\", \"planning\", \"execution\", \n",
    "    \"monitoring\", \"reflection\", \"adaptation\", \"error_recovery\", \"end\"\n",
    "]\n",
    "\n",
    "# Cognitive cycle step 1: Perception\n",
    "def perception_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process new inputs and update working memory with relevant perceptions.\"\"\"\n",
    "    \n",
    "    perception_prompt = \"\"\"You are the perception component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Process all new inputs from the user or environment\n",
    "    2. Identify salient features and important information\n",
    "    3. Integrate this information into working memory\n",
    "    4. Tag information with metadata for retrieval and processing\n",
    "    \n",
    "    Recent messages:\n",
    "    {{recent_messages}}\n",
    "    \n",
    "    Current working memory:\n",
    "    {{working_memory}}\n",
    "    \n",
    "    Process new perceptions without interpretation. Focus only on what is directly observed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract recent messages\n",
    "    recent_messages = []\n",
    "    for msg in state[\"messages\"][-5:]:\n",
    "        if isinstance(msg, (HumanMessage, AIMessage)):\n",
    "            recent_messages.append({\"role\": \"human\" if isinstance(msg, HumanMessage) else \"ai\", \n",
    "                                  \"content\": msg.content})\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = perception_prompt.replace(\"{{recent_messages}}\", json.dumps(recent_messages, indent=2))\n",
    "    filled_prompt = filled_prompt.replace(\"{{working_memory}}\", json.dumps(state.get(\"working_memory\", {}), indent=2))\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update working memory with new perceptions\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"current_perceptions\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"working_memory\": working_memory,\n",
    "        \"controller_state\": \"interpretation\"\n",
    "    }\n",
    "\n",
    "# Cognitive cycle step 2: Interpretation\n",
    "def interpretation_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Interpret perceptions in context of prior knowledge.\"\"\"\n",
    "    \n",
    "    interpretation_prompt = \"\"\"You are the interpretation component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Analyze current perceptions in the context of prior knowledge\n",
    "    2. Infer user's intent, emotional state, and fitness needs\n",
    "    3. Update user model with new interpretations\n",
    "    4. Identify potential knowledge gaps to address\n",
    "    \n",
    "    Current perceptions:\n",
    "    {{current_perceptions}}\n",
    "    \n",
    "    User model:\n",
    "    {{user_model}}\n",
    "    \n",
    "    Semantic memory (knowledge):\n",
    "    {{semantic_memory}}\n",
    "    \n",
    "    Provide a structured interpretation that connects perceptions to existing knowledge.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = interpretation_prompt.replace(\n",
    "        \"{{current_perceptions}}\", \n",
    "        json.dumps(state.get(\"working_memory\", {}).get(\"current_perceptions\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{user_model}}\", \n",
    "        json.dumps(state.get(\"user_model\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{semantic_memory}}\", \n",
    "        json.dumps(state.get(\"semantic_memory\", {}), indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update working memory with interpretation\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"current_interpretation\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content\n",
    "    }\n",
    "    \n",
    "    # Update user model based on interpretation\n",
    "    user_model = state.get(\"user_model\", {})\n",
    "    \n",
    "    # In a real system, we'd parse the interpretation to update the user model\n",
    "    # Here we're just tracking that we did the interpretation\n",
    "    user_model[\"last_interpretation\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"working_memory\": working_memory,\n",
    "        \"user_model\": user_model,\n",
    "        \"controller_state\": \"planning\"\n",
    "    }\n",
    "\n",
    "# Cognitive cycle step 3: Planning\n",
    "def planning_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Develop multi-level plans based on interpreted information.\"\"\"\n",
    "    \n",
    "    planning_prompt = \"\"\"You are the planning component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Develop a hierarchical plan to address the user's needs\n",
    "    2. Create high-level fitness goals broken down into actionable steps\n",
    "    3. Incorporate contingencies for potential obstacles\n",
    "    4. Ensure plans align with scientific principles and the user's constraints\n",
    "    \n",
    "    Current interpretation:\n",
    "    {{current_interpretation}}\n",
    "    \n",
    "    User model:\n",
    "    {{user_model}}\n",
    "    \n",
    "    Existing plans:\n",
    "    {{current_plan}}\n",
    "    \n",
    "    Develop a structured plan with clear objectives, actions, and success criteria.\n",
    "    Your plan should include short-term actions and long-term strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = planning_prompt.replace(\n",
    "        \"{{current_interpretation}}\", \n",
    "        json.dumps(state.get(\"working_memory\", {}).get(\"current_interpretation\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{user_model}}\", \n",
    "        json.dumps(state.get(\"user_model\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{current_plan}}\", \n",
    "        json.dumps(state.get(\"current_plan\", {}), indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update current plan\n",
    "    current_plan = state.get(\"current_plan\", {})\n",
    "    current_plan[\"plan_content\"] = response.content\n",
    "    current_plan[\"created_at\"] = datetime.now().isoformat()\n",
    "    current_plan[\"version\"] = current_plan.get(\"version\", 0) + 1\n",
    "    \n",
    "    # Update working memory with plan\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"current_plan_summary\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content[:500] + \"...\" if len(response.content) > 500 else response.content\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"working_memory\": working_memory,\n",
    "        \"current_plan\": current_plan,\n",
    "        \"controller_state\": \"execution\"\n",
    "    }\n",
    "\n",
    "# Cognitive cycle step 4: Execution\n",
    "def execution_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute the plan and interact with the user or environment.\"\"\"\n",
    "    \n",
    "    execution_prompt = \"\"\"You are the execution component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Implement the current plan through direct communication with the user\n",
    "    2. Use appropriate tools to update workout routines or retrieve information\n",
    "    3. Maintain a supportive, expert tone aligned with fitness coaching best practices\n",
    "    4. Document actions taken for monitoring and reflection\n",
    "    \n",
    "    Current plan to execute:\n",
    "    {{current_plan}}\n",
    "    \n",
    "    User model:\n",
    "    {{user_model}}\n",
    "    \n",
    "    Respond to the user directly, implementing the current plan in a natural, conversational manner.\n",
    "    Use scientific knowledge and motivational techniques appropriate to the user's needs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = execution_prompt.replace(\n",
    "        \"{{current_plan}}\", \n",
    "        json.dumps(state.get(\"current_plan\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{user_model}}\", \n",
    "        json.dumps(state.get(\"user_model\", {}), indent=2)\n",
    "    )\n",
    "    \n",
    "    # Only respond if the last message is from the user\n",
    "    last_message_is_user = False\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_message_is_user = True\n",
    "            break\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            last_message_is_user = False\n",
    "            break\n",
    "    \n",
    "    if last_message_is_user:\n",
    "        messages = state[\"messages\"] + [SystemMessage(content=filled_prompt)]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        \n",
    "        # Record execution in trace\n",
    "        execution_trace = state.get(\"execution_trace\", [])\n",
    "        execution_trace.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"action\": \"user_communication\",\n",
    "            \"content\": response.content,\n",
    "            \"plan_reference\": state.get(\"current_plan\", {}).get(\"version\")\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "            \"execution_trace\": execution_trace,\n",
    "            \"controller_state\": \"monitoring\"\n",
    "        }\n",
    "    else:\n",
    "        # No response needed, move to monitoring\n",
    "        return {\n",
    "            **state,\n",
    "            \"controller_state\": \"monitoring\"\n",
    "        }\n",
    "\n",
    "# Cognitive cycle step 5: Monitoring\n",
    "def monitoring_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Monitor execution and detect deviations or errors.\"\"\"\n",
    "    \n",
    "    monitoring_prompt = \"\"\"You are the monitoring component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Evaluate the execution against the planned objectives\n",
    "    2. Detect deviations, errors, or unexpected user responses\n",
    "    3. Assess user engagement and emotional response\n",
    "    4. Identify opportunities for adaptation\n",
    "    \n",
    "    Current plan:\n",
    "    {{current_plan}}\n",
    "    \n",
    "    Recent execution:\n",
    "    {{recent_execution}}\n",
    "    \n",
    "    User responses:\n",
    "    {{user_responses}}\n",
    "    \n",
    "    Provide a structured assessment of execution quality and any issues detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract user responses\n",
    "    user_responses = []\n",
    "    for msg in state[\"messages\"][-3:]:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            user_responses.append(msg.content)\n",
    "    \n",
    "    # Get recent execution from trace\n",
    "    recent_execution = state.get(\"execution_trace\", [])[-3:] if state.get(\"execution_trace\") else []\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = monitoring_prompt.replace(\n",
    "        \"{{current_plan}}\", \n",
    "        json.dumps(state.get(\"current_plan\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{recent_execution}}\", \n",
    "        json.dumps(recent_execution, indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{user_responses}}\", \n",
    "        json.dumps(user_responses, indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Determine if there's an error or if we should reflect\n",
    "    error_detected = \"error\" in response.content.lower() or \"issue\" in response.content.lower()\n",
    "    \n",
    "    # Update working memory\n",
    "    working_memory = state.get(\"working_memory\", {})\n",
    "    working_memory[\"monitoring_results\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content,\n",
    "        \"error_detected\": error_detected\n",
    "    }\n",
    "    \n",
    "    # Set next state based on monitoring results\n",
    "    next_state = \"error_recovery\" if error_detected else \"reflection\"\n",
    "    \n",
    "    # If error detected, capture error state\n",
    "    error_state = None\n",
    "    if error_detected:\n",
    "        error_state = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"description\": response.content,\n",
    "            \"error_context\": {\n",
    "                \"current_plan\": state.get(\"current_plan\", {}),\n",
    "                \"recent_messages\": [m.content for m in state[\"messages\"][-3:] if isinstance(m, (HumanMessage, AIMessage))]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"working_memory\": working_memory,\n",
    "        \"error_state\": error_state,\n",
    "        \"controller_state\": next_state\n",
    "    }\n",
    "\n",
    "# Cognitive cycle step 6: Reflection\n",
    "def reflection_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Reflect on performance and identify improvements.\"\"\"\n",
    "    \n",
    "    reflection_prompt = \"\"\"You are the reflection component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Evaluate overall performance and effectiveness\n",
    "    2. Identify patterns across multiple interactions\n",
    "    3. Recognize opportunities for knowledge enhancement\n",
    "    4. Develop specific improvements to agent behavior\n",
    "    \n",
    "    Recent monitoring results:\n",
    "    {{monitoring_results}}\n",
    "    \n",
    "    Execution history:\n",
    "    {{execution_trace}}\n",
    "    \n",
    "    Metacognition state:\n",
    "    {{metacognition}}\n",
    "    \n",
    "    Provide thoughtful reflection on system performance and concrete improvement ideas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = reflection_prompt.replace(\n",
    "        \"{{monitoring_results}}\", \n",
    "        json.dumps(state.get(\"working_memory\", {}).get(\"monitoring_results\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{execution_trace}}\", \n",
    "        json.dumps(state.get(\"execution_trace\", [])[-5:], indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{metacognition}}\", \n",
    "        json.dumps(state.get(\"metacognition\", {}), indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update reflection log\n",
    "    reflection_log = state.get(\"reflection_log\", [])\n",
    "    reflection_log.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content\n",
    "    })\n",
    "    \n",
    "    # Update metacognition with insights\n",
    "    metacognition = state.get(\"metacognition\", {})\n",
    "    metacognition[\"last_reflection\"] = datetime.now().isoformat()\n",
    "    metacognition[\"improvement_ideas\"] = metacognition.get(\"improvement_ideas\", [])\n",
    "    metacognition[\"improvement_ideas\"].append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content\n",
    "    })\n",
    "    \n",
    "    # Decide whether to adapt or end\n",
    "    adapt_needed = \"adapt\" in response.content.lower() or \"improve\" in response.content.lower() or \"change\" in response.content.lower()\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"reflection_log\": reflection_log,\n",
    "        \"metacognition\": metacognition,\n",
    "        \"controller_state\": \"adaptation\" if adapt_needed else \"end\"\n",
    "    }\n",
    "\n",
    "# Cognitive cycle step 7: Adaptation\n",
    "def adaptation_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Adapt behavior, knowledge, or plans based on reflection.\"\"\"\n",
    "    \n",
    "    adaptation_prompt = \"\"\"You are the adaptation component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Implement changes based on reflection insights\n",
    "    2. Update domain knowledge with new information\n",
    "    3. Adjust the user model to reflect new understanding\n",
    "    4. Refine plans to better meet objectives\n",
    "    \n",
    "    Recent reflection:\n",
    "    {{recent_reflection}}\n",
    "    \n",
    "    Current plan:\n",
    "    {{current_plan}}\n",
    "    \n",
    "    User model:\n",
    "    {{user_model}}\n",
    "    \n",
    "    Specify concrete changes to implement across system components.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get most recent reflection\n",
    "    recent_reflection = state.get(\"reflection_log\", [])[-1] if state.get(\"reflection_log\") else {}\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = adaptation_prompt.replace(\n",
    "        \"{{recent_reflection}}\", \n",
    "        json.dumps(recent_reflection, indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{current_plan}}\", \n",
    "        json.dumps(state.get(\"current_plan\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{user_model}}\", \n",
    "        json.dumps(state.get(\"user_model\", {}), indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update plan with adaptations\n",
    "    current_plan = state.get(\"current_plan\", {})\n",
    "    current_plan[\"adaptations\"] = current_plan.get(\"adaptations\", [])\n",
    "    current_plan[\"adaptations\"].append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"content\": response.content\n",
    "    })\n",
    "    current_plan[\"version\"] = current_plan.get(\"version\", 0) + 0.1\n",
    "    \n",
    "    # Update user model based on adaptation insights\n",
    "    user_model = state.get(\"user_model\", {})\n",
    "    user_model[\"last_adaptation\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"current_plan\": current_plan,\n",
    "        \"user_model\": user_model,\n",
    "        \"controller_state\": \"perception\"  # Return to perception to start a new cycle\n",
    "    }\n",
    "\n",
    "# Error recovery module\n",
    "def error_recovery_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Handle detected errors with graceful recovery.\"\"\"\n",
    "    \n",
    "    recovery_prompt = \"\"\"You are the error recovery component of an advanced fitness training system.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Analyze the detected error and its severity\n",
    "    2. Develop an appropriate recovery strategy\n",
    "    3. Implement immediate corrections in user communication\n",
    "    4. Update the system to prevent similar errors\n",
    "    \n",
    "    Error details:\n",
    "    {{error_state}}\n",
    "    \n",
    "    Current plan:\n",
    "    {{current_plan}}\n",
    "    \n",
    "    Recent messages:\n",
    "    {{recent_messages}}\n",
    "    \n",
    "    Develop a recovery plan that addresses the error while maintaining user trust.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract recent messages\n",
    "    recent_messages = []\n",
    "    for msg in state[\"messages\"][-5:]:\n",
    "        if isinstance(msg, (HumanMessage, AIMessage)):\n",
    "            recent_messages.append({\"role\": \"human\" if isinstance(msg, HumanMessage) else \"ai\", \n",
    "                                  \"content\": msg.content})\n",
    "    \n",
    "    # Fill prompt template\n",
    "    filled_prompt = recovery_prompt.replace(\n",
    "        \"{{error_state}}\", \n",
    "        json.dumps(state.get(\"error_state\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{current_plan}}\", \n",
    "        json.dumps(state.get(\"current_plan\", {}), indent=2)\n",
    "    )\n",
    "    filled_prompt = filled_prompt.replace(\n",
    "        \"{{recent_messages}}\", \n",
    "        json.dumps(recent_messages, indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [SystemMessage(content=filled_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Implement recovery by directly responding to user\n",
    "    recovery_message = f\"{response.content}\"\n",
    "    \n",
    "    # Record recovery action\n",
    "    execution_trace = state.get(\"execution_trace\", [])\n",
    "    execution_trace.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"action\": \"error_recovery\",\n",
    "        \"content\": recovery_message,\n",
    "        \"error_reference\": state.get(\"error_state\", {}).get(\"timestamp\")\n",
    "    })\n",
    "    \n",
    "    # Update metacognition with error handling\n",
    "    metacognition = state.get(\"metacognition\", {})\n",
    "    metacognition[\"error_history\"] = metacognition.get(\"error_history\", [])\n",
    "    metacognition[\"error_history\"].append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"error\": state.get(\"error_state\", {}),\n",
    "        \"recovery\": recovery_message\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=recovery_message)],\n",
    "        \"execution_trace\": execution_trace,\n",
    "        \"metacognition\": metacognition,\n",
    "        \"error_state\": None,  # Clear error state\n",
    "        \"controller_state\": \"reflection\"  # Move to reflection after recovery\n",
    "    }\n",
    "\n",
    "# Controller state router\n",
    "def controller_router(state: AgentState) -> str:\n",
    "    return state[\"controller_state\"]\n",
    "\n",
    "# Build the hierarchical cognitive graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add cognitive architecture nodes\n",
    "workflow.add_node(\"perception\", perception_node)\n",
    "workflow.add_node(\"interpretation\", interpretation_node)\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"execution\", execution_node)\n",
    "workflow.add_node(\"monitoring\", monitoring_node)\n",
    "workflow.add_node(\"reflection\", reflection_node)\n",
    "workflow.add_node(\"adaptation\", adaptation_node)\n",
    "workflow.add_node(\"error_recovery\", error_recovery_node)\n",
    "workflow.add_node(\"tools\", ToolNode([\n",
    "    retrieve_from_rag, \n",
    "    tool_fetch_workouts,\n",
    "    tool_get_workout_count,\n",
    "    tool_fetch_routines,\n",
    "    tool_update_routine,\n",
    "    tool_create_routine\n",
    "]))\n",
    "\n",
    "# Connect cognitive cycle\n",
    "workflow.add_conditional_edges(\n",
    "    \"perception\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"interpretation\": \"interpretation\",\n",
    "        \"planning\": \"planning\",\n",
    "        \"execution\": \"execution\",\n",
    "        \"monitoring\": \"monitoring\",\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"error_recovery\": \"error_recovery\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"interpretation\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"execution\": \"execution\",\n",
    "        \"monitoring\": \"monitoring\",\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"error_recovery\": \"error_recovery\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"planning\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"execution\": \"execution\",\n",
    "        \"monitoring\": \"monitoring\",\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"error_recovery\": \"error_recovery\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"execution\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"monitoring\": \"monitoring\",\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"error_recovery\": \"error_recovery\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"monitoring\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"error_recovery\": \"error_recovery\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"adaptation\": \"adaptation\",\n",
    "        \"perception\": \"perception\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"adaptation\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"perception\": \"perception\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"error_recovery\",\n",
    "    controller_router,\n",
    "    {\n",
    "        \"reflection\": \"reflection\",\n",
    "        \"perception\": \"perception\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add tool usage for applicable nodes\n",
    "for node in [\"execution\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        tools_condition,\n",
    "        {\n",
    "            True: \"tools\",\n",
    "            False: node\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Tool returns to the execution node\n",
    "workflow.add_edge(\"tools\", \"execution\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"perception\")\n",
    "\n",
    "# Compile the graph\n",
    "level5_app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "level4_diagram = level4_app.get_graph().draw_mermaid()\n",
    "level5_diagram = level5_app.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tmemory_manager(memory_manager)\n",
      "\treasoning_engine(reasoning_engine)\n",
      "\tuser_modeler(user_modeler)\n",
      "\tcoordinator(coordinator)\n",
      "\tprofiler_agent(profiler_agent)\n",
      "\tresearch_agent(research_agent)\n",
      "\tplanner_agent(planner_agent)\n",
      "\tanalyst_agent(analyst_agent)\n",
      "\tadaptation_agent(adaptation_agent)\n",
      "\tcoach_agent(coach_agent)\n",
      "\ttools(tools)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> memory_manager;\n",
      "\tadaptation_agent --> coordinator;\n",
      "\tanalyst_agent --> coordinator;\n",
      "\tcoach_agent --> coordinator;\n",
      "\tmemory_manager --> user_modeler;\n",
      "\tplanner_agent --> coordinator;\n",
      "\tprofiler_agent --> coordinator;\n",
      "\tresearch_agent --> coordinator;\n",
      "\tuser_modeler --> reasoning_engine;\n",
      "\treasoning_engine -.-> coordinator;\n",
      "\treasoning_engine -.-> profiler_agent;\n",
      "\treasoning_engine -.-> research_agent;\n",
      "\treasoning_engine -.-> planner_agent;\n",
      "\treasoning_engine -.-> analyst_agent;\n",
      "\treasoning_engine -.-> adaptation_agent;\n",
      "\treasoning_engine -.-> coach_agent;\n",
      "\tcoordinator -.-> memory_manager;\n",
      "\tcoordinator -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tcoordinator -. &nbsp;True&nbsp; .-> tools;\n",
      "\tprofiler_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tresearch_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tplanner_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tanalyst_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tadaptation_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\tcoach_agent -. &nbsp;True&nbsp; .-> tools;\n",
      "\ttools -.-> coordinator;\n",
      "\ttools -.-> profiler_agent;\n",
      "\ttools -.-> research_agent;\n",
      "\ttools -.-> planner_agent;\n",
      "\ttools -.-> analyst_agent;\n",
      "\ttools -.-> adaptation_agent;\n",
      "\ttools -.-> coach_agent;\n",
      "\tcoordinator -. &nbsp;False&nbsp; .-> coordinator;\n",
      "\tprofiler_agent -. &nbsp;False&nbsp; .-> profiler_agent;\n",
      "\tresearch_agent -. &nbsp;False&nbsp; .-> research_agent;\n",
      "\tplanner_agent -. &nbsp;False&nbsp; .-> planner_agent;\n",
      "\tanalyst_agent -. &nbsp;False&nbsp; .-> analyst_agent;\n",
      "\tadaptation_agent -. &nbsp;False&nbsp; .-> adaptation_agent;\n",
      "\tcoach_agent -. &nbsp;False&nbsp; .-> coach_agent;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(level4_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tperception(perception)\n",
      "\tinterpretation(interpretation)\n",
      "\tplanning(planning)\n",
      "\texecution(execution)\n",
      "\tmonitoring(monitoring)\n",
      "\treflection(reflection)\n",
      "\tadaptation(adaptation)\n",
      "\terror_recovery(error_recovery)\n",
      "\ttools(tools)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> perception;\n",
      "\ttools --> execution;\n",
      "\tperception -.-> interpretation;\n",
      "\tperception -.-> planning;\n",
      "\tperception -.-> execution;\n",
      "\tperception -.-> monitoring;\n",
      "\tperception -.-> reflection;\n",
      "\tperception -.-> adaptation;\n",
      "\tperception -.-> error_recovery;\n",
      "\tperception -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tinterpretation -.-> planning;\n",
      "\tinterpretation -.-> execution;\n",
      "\tinterpretation -.-> monitoring;\n",
      "\tinterpretation -.-> reflection;\n",
      "\tinterpretation -.-> adaptation;\n",
      "\tinterpretation -.-> error_recovery;\n",
      "\tinterpretation -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tplanning -.-> execution;\n",
      "\tplanning -.-> monitoring;\n",
      "\tplanning -.-> reflection;\n",
      "\tplanning -.-> adaptation;\n",
      "\tplanning -.-> error_recovery;\n",
      "\tplanning -. &nbsp;end&nbsp; .-> __end__;\n",
      "\texecution -.-> monitoring;\n",
      "\texecution -.-> reflection;\n",
      "\texecution -.-> adaptation;\n",
      "\texecution -.-> error_recovery;\n",
      "\texecution -. &nbsp;end&nbsp; .-> __end__;\n",
      "\texecution -. &nbsp;True&nbsp; .-> tools;\n",
      "\tmonitoring -.-> reflection;\n",
      "\tmonitoring -.-> adaptation;\n",
      "\tmonitoring -.-> error_recovery;\n",
      "\tmonitoring -. &nbsp;end&nbsp; .-> __end__;\n",
      "\treflection -.-> adaptation;\n",
      "\treflection -.-> perception;\n",
      "\treflection -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tadaptation -.-> perception;\n",
      "\tadaptation -. &nbsp;end&nbsp; .-> __end__;\n",
      "\terror_recovery -.-> reflection;\n",
      "\terror_recovery -.-> perception;\n",
      "\terror_recovery -. &nbsp;end&nbsp; .-> __end__;\n",
      "\texecution -. &nbsp;False&nbsp; .-> execution;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(level5_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
