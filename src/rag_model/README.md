
## Usage within Airflow

The RAG evaluation process is typically run as part of the `model_evaluation_pipeline_dag.py`.

1.  The `run_rag_evaluation` task (usually a `PythonOperator`) is defined in the DAG.
2.  This task calls a wrapper function (`run_rag_eval_wrapper`).
3.  The wrapper function:
    *   Checks for required environment variables.
    *   Instantiates the `AdvancedRAGEvaluator` from `src.rag_model.advanced_rag_evaluation`.
    *   Calls the `evaluator.compare_implementations()` method.
4.  The `compare_implementations` method:
    *   Iterates through the initialized RAG models (`advanced`, `modular`, etc.).
    *   For each RAG model, it runs test queries, generates responses, and calculates evaluation metrics using the `evaluate_response` method.
    *   Logs parameters, average metrics, and detailed results for **each implementation** to MLflow using the `MLflowRAGTracker`.
    *   Saves the **final comparison summary** (including the best implementation) to a JSON file in the specified GCS bucket (`EVALUATION_OUTPUT_BUCKET`).
    *   Prints a summary to the Airflow task logs.

## Running Evaluation Standalone (for Testing)

You can test the evaluation script directly (outside Airflow) if needed:

1.  Ensure you are in an environment with all dependencies installed.
2.  Set up Application Default Credentials for GCS access (`gcloud auth application-default login`).
3.  Set all required environment variables (API Keys, MLflow URI, Bucket names).
4.  Ensure the MLflow server is running and accessible.
5.  Run the script (adjust path as necessary):
    ```bash
    python src/rag_model/advanced_rag_evaluation.py --implementation all --output-dir /tmp/eval_results
    ```
    *(Note: The `--output-dir` might only be used for temporary files if GCS saving is enabled).*

## File Structure

-   `__init__.py`: Package init.
-   `naive_rag.py`: Naive RAG implementation.
-   `advanced_rag.py`: Advanced RAG implementation.
-   `modular_rag.py`: Modular RAG implementation.
-   `graph_rag.py`: Graph RAG implementation.
-   `raptor_rag.py`: RAPTOR RAG implementation.
-   `advanced_rag_evaluation.py`: Core evaluation logic and MLflow/GCS integration.
-   `compare_rag_implementations.py`: (Potentially outdated) Script to run comparisons - functionality now likely within `advanced_rag_evaluation.py`.
-   `rag_integration.py`: (Potentially outdated) Script for integrating RAG - check current usage.
-   `mlflow/`: MLflow specific utilities.
    -   `__init__.py`
    -   `mlflow_rag_tracker.py`: Class to interact with MLflow.
    -   `mlflow_rag_metrics.py`: (Optional) Metric definitions.
-   `README.md`: This file.
-   `rag_implementation_strategy.md`: Strategy details.

## Notes

*   Ensure the Pinecone index specified by `PINECONE_INDEX_NAME` exists and is populated with embeddings generated by the *same* embedding model (`sentence-transformers/all-mpnet-base-v2`) used in the evaluation script.
*   Regularly review the effectiveness of the evaluation metrics and the judge LLM prompts.
*   Monitor MLflow runs and GCS output bucket for evaluation results.
